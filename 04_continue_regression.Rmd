---
title: "Expanding on Simple Regression Models"
output: 
html_document: 
toc: true 
toc_depth: 2
---


# Objectives

- Adding a continuous predictor
- Interpret effects of multiple predictors in a regression.


```{r load_packages, message = F, results = 'hide'}

library(tidyverse)
library(here)
library(janitor)
library(purrr)
library(viridis)
library(RColorBrewer)

library(brms)
library(tidybayes)
library(bayesplot)

library(knitr)
library(kableExtra)

theme_set(theme_bw())

```


# Introduction

In a previous lesson we introduced linear regression with a single, binary predictor. In this lesson we will be expanding on this by introducing continuous predictors, and interpreting models with more than one predictor. 


## Our first example

For this lesson we will be using fitting models to data from the [PanTHERIA}](http://www.esapubs.org/archive/ecol/E090/184/default.htm) database, a large collection of trait data for extant mammals. For a more detailed explanation of the dataset and each variable, you can review the [data dictionary](http://esapubs.org/archive/ecol/E090/184/metadata.htm). A key detail to PanTHERIA is that missing data is coded as -999.00 and not as NA or blank cells. This knowledge is something we can use when importing our data so that it translates easily into the R environment. Also, a lot of the variables have terrible names with mixed unction, capital letters, and begin with numbers -- all of these are very bad to program around and need to be dealt with (using the `janitor` package).

That all being said, let's import the dataset, clean it up a bit, and then start visualizing it.
```{r pantheria}

pantheria <- read_tsv(here('PanTHERIA_1-0_WR05_Aug2008.txt'), 
                      na = '-999.00') %>%
clean_names() %>% 
mutate(mass_log = log(x5_1_adult_body_mass_g),
       range_group_log = log(x22_1_home_range_km2),
       range_indiv_log = log(x22_2_home_range_indiv_km2),
       density_log = log(x21_1_population_density_n_km2),
       activity_cycle = case_when(x1_1_activity_cycle == 1 ~ 'nocturnal',
                                  x1_1_activity_cycle == 2 ~ 'mixed',
                                  x1_1_activity_cycle == 3 ~ 'diurnal'),
       trophic_level = case_when(x6_2_trophic_level == 1 ~ 'herbivore',
                                 x6_2_trophic_level == 2 ~ 'omnivore',
                                 x6_2_trophic_level == 2 ~ 'carnivore'))

```

```{r pantheria_plot1}

pantheria %>%
  drop_na(activity_cycle) %>%
  ggplot(aes(x = trophic_level, 
             y = range_group_log)) +
geom_violin(fill = 'grey80', 
            draw_quantiles = c(0.1, 0.5, 0.9)) +
geom_jitter(height = 0,
            alpha = 0.5,
            mapping = aes(colour = msw05_order)) +
scale_colour_viridis(discrete = TRUE, name = 'Order') +
labs(x = 'Activity Cycle', 
     y = expression(paste('Group range size ', log(km^2))),
     title = 'Group range size differences between trophic levels, \norders highlighted')

```

```{r pantheria_plot2}

pantheria %>%
  ggplot(aes(x = mass_log, y = density_log, colour = msw05_order)) +
  geom_point() +
  scale_colour_viridis(discrete = TRUE, name = 'Order') +
  labs(x = expression(paste('Body mass ', log(g^2))),
       y = expression(paste('Population density ', log(n / km^2))),
       title = 'Body mass and population density, orders highlighted')

```


There are tons of ways we could deconstruct this dataset, some much more logical than others. For this tutorial, we're going to focus on trying to understand how population density varies across mammals. There are tons of factors that can influence the population density of a species, so our process will be to slowly build up a model one predictor at a time.

Let's begin our model in a similar fashion to our previous one, with an intercept-only model. We can then add our first continuous predictor from there.

Our response variable is `density_log` is a continuous value from $-\infty$ to $\infty$ so it is probably a good place to start by assuming it could be approximated with a normal distribution, as is common with linear regression. The normal distribution as defined in `brms` has two parameters: mean and standard deviation. For our simple intercept-only model, we do not need to expand on these parameters -- after all, the intercept describes the average value or *mean* of the response.

Let's write this out. Let $y$ be `density_log`, $\mu$ be the mean of `density_log`, and $\sigma$ be the standard deviation of `density_log`.

$$
y \sim \text{Normal}(\mu, \sigma)
$$
Can you recall what all the of the parts of the above statement mean? What are we missing? Priors!

We can probably stick with pretty vague priors here -- the mean is probably somewhere between -10 and 10 log(millimeters) and probably has at least that much range. Here's my starting point. Could I improve it? Do we have enough data that it probably won't matter?

$$
\begin{align}
y &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(0, 10) \\
\mu &\sim \text{Cauchy}^{+}(0, 10) \\
\end{align}
$$

Something is new here -- what is this Cauchy$^{+}$ distribution? What happened to our uniform prior we used last lesson? 

Let's implement this model in `brms`. We are going to need to ignore species that have missing data for `density_log` -- something `brms()` does automatically. Is this choice ideal? Let's assume it is for now, but if you are interested in learning more about handling missing values in our models, look up **data imputation** -- this is an advanced topic we will not be covering in these introductory lessons.


```{r fit_intercept, cache = TRUE, message = FALSE, results = 'hide'}

m_1 <- 
  pantheria %>%
  drop_na(density_log) %>%
  brm(data = .,
      family = gaussian(),
      formula = bf(density_log ~ 1),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000,
      warmup = 1000,
      chains = 4,
      cores = 4,
      refresh = 0)

```

```{r intercept_print}

print(m_1)

```


As with any model, we should see how well it describes our data -- maybe this simple model does a good enough job?

Let's compare our observed distribution of population densities versus our posterior predictive distribution.

```{r intercept_ppc}

pantheria %>%
  drop_na(density_log) %>%
  add_predicted_draws(model = m_1,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = .prediction, group = .draw)) +
  geom_line(stat = 'density', 
            alpha = 0.1,
            colour = 'blue') +
geom_line(stat = 'density',
          data = pantheria %>% drop_na(density_log, mass_log),
          mapping = aes(x = density_log,
                        group = NULL),
          colour = 'black',
          size = 1.5) +
scale_y_continuous(NULL, breaks = NULL) +
labs(x = expression(paste('Population density  ', log(n / km^2))),
     title = 'Population density, actual versus predicted.') +
NULL

```

While we might be describing the overall mean and standard deviation of our data, I do not think our simple model is capable of capturing the a lot of the complexity in our data. Our data appears to be multimodal and has a very different spread, especially on the right-hand side. We are going to have to include more information we if want to better describe our data.



## Adding a single continuous predictor

Just like in our previous lesson, to improve our model we're going to add a single predictor. What's new to this lesson is that that predictor is a continuous variable: average individual mass in log grams or `mass_log`. 

In linear regression, our predictors tend to describe change in mean $y$ has a function of an intercept, one or more regression coefficients, and one or more predictor. 

To do this, we need to define $\mu$ as a function of our predictor. Do you remember from last lesson how we did this? Try writing out a model definition by hand.

First, let's define $x$ as `mass_log`. Also, let's define two more variables: let $\alpha$ be the intercept of our regression, and let $\beta$ be the regression coefficient for $x$. Given this new information, here is how we can write out our regression model.
$$
\begin{align}
y_{i} &\sim \text{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \beta x_{i} \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Normal}(-1, 5) \\
\sigma &\sim \text{Cauchy}^{+}(0, 5) \\
\end{align}
$$

Are the choice of priors reasonable? Take a closer look at the prior for $\beta$ -- this is a **weakly informative prior**. I'm guessing that the slope is probably negative, but allow for the possibility of a 0 or positive slope -- albeit less than that of a negative slope. Is this justified? Think of the physical definition of the variables -- what would you guess the relationship between body size and population density to be?

You might notice this model is functionally identical to the model from our previous lesson where we had only a single binary predictor. The difference is all on the data end, and not the model, as $x$ can take on any value and not just 0 and 1.

How do we interpret all of these parameters? Our previous lesson gave us all the information we needed to describe each parameter, but I will reiterate them here because this is really important. If we don't know what our parameters precisely mean, we cannot interpret them.

- $\mu$ average value of $y$
- $\sigma$ standard deviation of $y$
- $\alpha$ intercept, average value of $y$ when $x$ = 0
- $\beta$ slope, expected change in $y$ per unit change in $x$

Let's implement this model in `brms`. Like before, we are going to ignore species that have missing data for either `density_log` or `mass_log` -- `brms()` does this for us automatically. 

```{r fit_predictor, cache = TRUE, message = FALSE, results = 'hide'}

m_2 <- 
  pantheria %>%
  drop_na(density_log, mass_log) %>%
  brm(data = .,
      family = gaussian(),
      formula = bf(density_log ~ 1 + mass_log),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(-1, 10), class = b),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000,
      warmup = 1000,
      chains = 4,
      cores = 4,
      refresh = 0)

```

```{r predictor_print}

print(m_2)

```

Now let's see how much adding this predictor improves our ability to describe $y$. We can also visualize our data as a scatter plot with the linear predictor overlain to demonstrate our model estimates of *mean* population density as a function of species mass.

```{r predictor_fitted}

pantheria %>%
  drop_na(density_log, mass_log) %>%
  add_fitted_draws(model = m_2,
                   n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = mass_log, y = density_log)) +
  geom_line(mapping = aes(y = .value, group = .draw),
            alpha = 1 / 20,
            colour = brewer.pal(5, "Blues")[[5]]) +
geom_point(data = pantheria, size = 2) +
scale_fill_brewer()

```

The previous plot only covers our model's estimates for *mean* population density but this is not all our model is telling us. As with our earlier posterior predictive comparisons from the intercept-only model, we can use the full posterior predictive distribution to compare our observed data to 100 datasets drawn from the posterior predictive distribution. Because the posterior predictive distribution also takes into account the estimated scale of our data ($\sigma$) and thus estimates individual values of $y$ and not just the expected value of $y$, these types of comparisons give us a fuller appreciation of how well our model is or is not representing out data.

```{r predictor_predicted}

pantheria %>%
  drop_na(density_log, mass_log) %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = mass_log, y = density_log)) +
  stat_lineribbon(mapping = aes(y = .prediction),
                  .width = c(0.9, 0.5, 0.1),
                  colour = brewer.pal(5, "Blues")[[5]]) +
geom_point(data = pantheria, size = 2) +
scale_fill_brewer()

```

We can also do explicit posterior predictive tests to see how well our model captures specific parts of our data such as the overall density, the median, or differences between unmodeled classes. For our first posterior predictive test, let's do a comparison between the density of our data, $y$, and the densities of 100 simulated datasets drawn from our posterior predictive distribution, $y^{\tilde}$.

```{r predictor_dens_ppc}

pantheria %>%
  drop_na(density_log, mass_log) %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = .prediction, group = .draw)) +
  geom_line(stat = 'density', 
            alpha = 0.1,
            colour = 'blue') +
geom_line(stat = 'density',
          data = pantheria %>% drop_na(density_log, mass_log),
          mapping = aes(x = density_log,
                        group = NULL),
          colour = 'black',
          size = 1.5) +
scale_y_continuous(NULL, breaks = NULL) +
labs(x = expression(paste('Population density  ', log(n / km^2))),
     title = 'Population density, actual versus predicted.') +
NULL

```


```{r predictor_median_ppc}

pantheria_summary <-
  pantheria %>%
  drop_na(density_log, mass_log) %>%
  dplyr::summarize(mean = mean(density_log)) %>%
  pull()

pantheria_summary_ppc <- 
  pantheria %>%
  drop_na(density_log, mass_log) %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  group_by(.draw) %>%
  dplyr::summarize(density_median_ppc = median(.prediction))

med_ppc <- 
  pantheria_summary_ppc %>%
  dplyr::summarize(per = sum(density_median_ppc > pantheria_summary) / n()) %>%
  pull()

pantheria_summary_ppc %>%
  ggplot(aes(x = density_median_ppc)) +
  geom_histogram(fill = 'blue') +
  geom_vline(xintercept = pantheria_summary,
             size = 2) +
labs(subtitle = paste(med_ppc, '% of estimates greater than observed'))

```

```{r predictor_median_trophic_ppc}



```




Given all of the above plots, in particular the posterior predictive tests of how well our model reproduces the median value of the observed, it appears that while this model does slightly better job at approximating our data than our earlier model it still fails are adequately representing our data. We are failing to reproduce the multi-modality of the data. What could we do to improve our model?



### Centering

The intercept of a linear regression model is normally interpreted as the average value of $y$ when all predictors equal 0. A consequence of this definition means that the value of the intercept is frequently uninterpretable without also studying the regression coefficients. This is also the reason that we commonly need very weak priors for intercepts.

A trick for improving our interpretation of the intercept $\alpha$ is **centering** our (continuous) predictors. Centering is the procedure of subtracting the mean of a variable from each value. Namely:
```{r center}

pantheria <- 
  pantheria %>%
  mutate(mass_log_center = mass_log - mean(mass_log, na.rm = TRUE))

```

$\alpha$ is still the expected value of the outcome variable when the predictor is equal to zero. But now the mean value of the predictor is also zero. So the intercept all means: the expected value of the outcome, when the predictor is at its average value. This makes interpreting the intercept a lot easier.

To illustrate this, let's refit the model with the newly centered data.

```{r fit_center, cache = TRUE, message = FALSE, results = 'hide'}

m_3 <- 
  pantheria %>%
  drop_na(density_log, mass_log_center) %>%
  brm(data = .,
      family = gaussian(),
      formula = bf(density_log ~ 1 + mass_log_center),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(1, 5), class = b),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000,
      warmup = 1000,
      chains = 4,
      cores = 4,
      refresh = 0)

```

```{r center_print}

print(m_3)

```

Can you explain why centering changes the value of $\alpha$ but not $\beta$? 

Centering will not change our models posterior predictive performance, but really improves the interpretability of our model parameters. Centering can also be beneficial for estimating parameter values by decreasing posterior correlation amoung the parameters.

I recommend always centering your (continuous) predictors. 





# More than one predictor

Our posterior predictive analysis of our current model of population density demonstrates that it is not fully adequate at describing our data.



$$
\begin{align}
y_{i} &\sim \text{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \beta_{1} m_{i} + \beta_{2} o_{i} + \beta_{3} n_{i} \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Normal}(0, 5) \\
\sigma &\sim \text{Cauchy}^{+}(0, 5) \\
\end{align}
$$


```{r fit_multi, cache = TRUE, message = FALSE, results = 'hide'}

m_4 <- 
  pantheria %>%
  drop_na(density_log, mass_log_center, trophic_level) %>%
  brm(data = .,
      family = gaussian(),
      formula = bf(density_log ~ 1 + mass_log_center + trophic_level),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(normal(-1, 5), class = b, coef = mass_log_center),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000,
      warmup = 1000,
      chains = 4,
      cores = 4,
      refresh = 0)

```

```{r multi_print}

print(m_4)

```


## Scaling


```{r scale}

pantheria <- 
  pantheria %>%
  drop_na(mass_log) %>%
  mutate(mass_log_scale = scale(mass_log, center = TRUE, scale = TRUE))

```


```{r fit_multiscale, cache = TRUE, message = FALSE, results = 'hide'}

m_5 <- 
  pantheria %>%
  drop_na(density_log, mass_log_scale, trophic_level) %>%
  brm(data = .,
      family = gaussian(),
      formula = bf(density_log ~ 1 + mass_log_scale + trophic_level),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(normal(-1, 5), class = b, coef = mass_log_scale),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000,
      warmup = 1000,
      chains = 4,
      cores = 4,
      refresh = 0)

```

```{r multiscale_print}

print(m_5)

```


I recommend always rescaling your (continuous) predictors to have the same scale.
