---
title: "Expanding on Simple Regression Models"
output: 
  html_document: 
    toc: true 
    toc_depth: 2
---


# Objectives

- Adding a continuous predictor
- Interpret effects of multiple predictors in a regression.
- Interpret predictor interactions with both discrete and continuous covariates.
- Visualize interaction effects.


```{r load_packages, message = F, results = 'hide'}

library(tidyverse)
library(here)
library(janitor)
library(purrr)
library(viridis)
library(RColorBrewer)

library(brms)
library(tidybayes)

library(knitr)
library(kableExtra)

theme_set(theme_bw())

```


# Introduction

In a previous lesson we introduced linear regression with a single, binary predictor. In this lesson we will be exploring more complicated forms of linear regression including continuous predictors, multiple predictors, and interactions.



# Continuous predictors

In out last lesson on regression we focused on linear regression with a single binary predictor. Along the way we were introduced to a notation for clearly communicating statistical models, multiple ways of summarizing our regression model fit, and the basic differences between the linear predictor of the regression model and the posterior predictive distribution of the same model.

The first part of this lesson is devoted to introducing continuous predictors. Example continuous variables include measures such as height, wing length, beak depth, or shell volume.


## Our first example


In our analyses today we will be using the [PanTHERIA}](http://www.esapubs.org/archive/ecol/E090/184/default.htm) database, a large collection of trait data for extant mammals. You can review the [data dictionary](http://esapubs.org/archive/ecol/E090/184/metadata.htm) for explinations of each variable. Missing data is coded as -999.00, something we can use when importing our data. Also, a lot of the variables have terrible names with lots of punction, captial letters and numbers -- all of these are very bad to program around. 

That all being said, let's import the dataset, clean it up a bit, and then start visualizing it.
```{r pantheria}

pantheria <- read_tsv(here('PanTHERIA_1-0_WR05_Aug2008.txt'), 
											na = '-999.00') %>%
	clean_names() %>% 
	mutate(mass_log = log(x5_1_adult_body_mass_g),
         range_group_log = log(x22_1_home_range_km2),
         range_indiv_log = log(x22_2_home_range_indiv_km2),
         density_log = log(x21_1_population_density_n_km2),
         activity_cycle = case_when(x1_1_activity_cycle == 1 ~ 'nocturnal',
                                    x1_1_activity_cycle == 2 ~ 'mixed',
                                    x1_1_activity_cycle == 3 ~ 'diurnal')) 
```

```{r pantheria_plot1}

pantheria %>%
	drop_na(activity_cycle) %>%
	ggplot(aes(x = activity_cycle, 
						 y = range_group_log)) +
  geom_violin(fill = 'grey80', 
							draw_quantiles = c(0.1, 0.5, 0.9)) +
  geom_jitter(height = 0,
              alpha = 0.5,
              mapping = aes(colour = msw05_order)) +
  scale_colour_viridis(discrete = TRUE, name = 'Order') +
  labs(x = 'Activity Cycle', 
       y = expression(paste('Group range size ', log(km^2))),
       title = 'Group range size differences between activity cycles, \norders highlighted')

```

```{r pantheria_plot2}

pantheria %>%
  ggplot(aes(x = mass_log, y = density_log, colour = msw05_order)) +
  geom_point() +
  scale_colour_viridis(discrete = TRUE, name = 'Order') +
  labs(x = expression(paste('Body mass ', log(g^2))),
       y = expression(paste('Population density ', log(n / km^2))),
       title = 'Body mass and population density, orders highlighted')

```


There are tons of ways we could deconstruct this dataset, some much more logical than others. For this tutorial, we're going to focus on trying to understand how population density varies across mammals. There are tons of factors that can influence the population density of a species, so our process will be to slowly build up a model one predictor at a time.

Let's begin our model in a similar fashion to our previous one, with an intercept-only model. We can then add our first continuous predictor from there.

Our response variable is `density_log` is a continuous value from $-\infty$ to $\infty$ so it is probably a good place to start by assuming it could be approximated with a normal distribution, as is common with linear regression. The normal distribution as defined in `brms` has two parameters: mean and standard deviation. For our simple intercept-only model, we do not need to expand on these parameters -- after all, the intercept describes the average value or *mean* of the response.

Let's write this out. Let $y$ be `density_log`, $\mu$ be the mean of `density_log`, and $\sigma$ be the standard deviation of `density_log`.

$$
y \sim \text{Normal}(\mu, \sigma)
$$
Can you recall what all the of the parts of the above statement mean? What are we missing? Priors!

We can probably stick with pretty vague priors here -- the mean is probably somewhere between -10 and 10 log(millimeters) and probably has at least that much range. Here's my starting point. Could I improve it? Do we have enough data that it probably won't matter?

$$
\begin{align}
y &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(0, 10) \\
\mu &\sim \text{Cauchy}^{+}(0, 10) \\
\end{align}
$$

Something is new here -- what is this Cauchy$^{+}$ distribution? What happened to our uniform prior we used last lesson? 

Let's implement this model in `brms`. We are going to need to ignore species that have missing data for `density_log` -- something `brms()` does automatically. Is this choice ideal? Let's assume it is for now, but if you are interested in learning more about handling missing values in our models, look up **data imputation** -- this is an advanced topic we will not be covering in these introductory lessons.


```{r fit_intercept, cache = TRUE, message = FALSE, results = 'hide'}

m_1 <- brm(data = pantheria,
           family = gaussian(),
           formula = bf(density_log ~ 1),
           prior = c(prior(normal(0, 10), class = Intercept),
                     prior(cauchy(0, 5), class = sigma)),
           iter = 2000,
           warmup = 1000,
           chains = 4,
           cores = 4,
           refresh = 0)

```

```{r intercept_print}

print(m_1)

```


As with any model, we should see how well it describes our data -- maybe this simple model does a good enough job?

Let's compare our observed distribution of population densities versus our posterior predictive distribution.

```{r intercept_ppc}

pantheria %>%
  drop_na(density_log) %>%
  add_predicted_draws(model = m_1,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = .prediction, group = .draw)) +
  geom_line(stat = 'density', 
            alpha = 0.1,
            colour = 'blue') +
  geom_line(stat = 'density',
            data = pantheria %>% drop_na(density_log, mass_log),
            mapping = aes(x = density_log,
                          group = NULL),
            colour = 'black',
            size = 1.5) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = expression(paste('Population density  ', log(n / km^2))),
       title = 'Population density, actual versus predicted.') +
  NULL

```

While we might be describing the overall mean and standard deviation of our data, I do not think our simple model is capable of capturing the a lot of the complexity in our data. Our data appears to be multimodal and has a very different spread, especially on the right-hand side. We are going to have to include more information we if want to better describe our data.



## Adding a single continuous predictor

Just like in our previous lesson, to improve our model we're going to add a single predictor. What's new to this lesson is that that predictor is a continuous variable: average individual mass in log grams or `mass_log`. 

In linear regression, our predictors tend to describe change in mean $y$ has a function of an intercept, one or more regression coefficients, and one or more predictor. 

To do this, we need to define $\mu$ as a function of our predictor. Do you remember from last lesson how we did this? Try writing out a model definition by hand.

First, let's define $x$ as `mass_log`. Also, let's define two more variables: let $\alpha$ be the intercept of our regression, and let $\beta$ be the regression coefficient for $x$. Given this new information, here is how we can write out our regression model.
$$
\begin{align}
y_{i} &\sim \text{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \beta x_{i} \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Normal}(0, 5) \\
\sigma &\sim \text{Cauchy}^{+}(0, 5) \\
\end{align}
$$

You'll notice this model is functionally identical to our model with a single binary predictor. The difference here is that $x$ can take on values that are not just 0 and 1.

How do we interpret all of these parameters? Our previous lesson gave us all the information we needed to describe each parameter, but I will reiterate them here because this is really important. If we don't know what our parameters precisely mean, we cannot interpret them.

- $\mu$ average value of $y$
- $\sigma$ standard deviation of $y$
- $\alpha$ intercept, average value of $y$ when $x$ = 0
- $\beta$ slope, expected change in $y$ per unit change in $x$

Let's implement this model in `brms`. Like before, we are going to ignore species that have missing data for either `density_log` or `mass_log` -- `brms()` does this for us automatically. 

```{r fit_predictor, cache = TRUE, message = FALSE, results = 'hide'}

m_2 <- brm(data = pantheria,
           family = gaussian(),
           formula = bf(density_log ~ 1 + mass_log),
           prior = c(prior(normal(0, 10), class = Intercept),
                     prior(normal(0, 10), class = b),
                     prior(cauchy(0, 5), class = sigma)),
           iter = 2000,
           warmup = 1000,
           chains = 4,
           cores = 4,
           refresh = 0)

```

```{r predictor_print}

print(m_2)

```

Now let's see how much adding this predictor improves our ability to describe $y$. Let's look at this two ways -- posterior predictive comparison between densities of $y$ and $y^{\tilde}$, 


```{r predictor_ppc}

pantheria %>%
  drop_na(density_log, mass_log) %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = .prediction, group = .draw)) +
  geom_line(stat = 'density', 
            alpha = 0.1,
            colour = 'blue') +
  geom_line(stat = 'density',
            data = pantheria %>% drop_na(density_log, mass_log),
            mapping = aes(x = density_log,
                          group = NULL),
            colour = 'black',
            size = 1.5) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = expression(paste('Population density  ', log(n / km^2))),
       title = 'Population density, actual versus predicted.') +
  NULL

```

```{r predictor_fitted}

pantheria %>%
  drop_na(density_log, mass_log) %>%
  add_fitted_draws(model = m_2,
                   n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = mass_log, y = density_log)) +
  geom_line(mapping = aes(y = .value, group = .draw),
            alpha = 1 / 20,
            colour = brewer.pal(5, "Blues")[[5]]) +
  geom_point(data = pantheria, size = 2) +
  scale_fill_brewer()

```

```{r predictor_predicted}

pantheria %>%
  drop_na(density_log, mass_log) %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = mass_log, y = density_log)) +
  stat_lineribbon(mapping = aes(y = .prediction),
                  .width = c(0.9, 0.5, 0.1),
                  colour = brewer.pal(5, "Blues")[[5]]) +
  geom_point(data = pantheria, size = 2) +
  scale_fill_brewer()

```







It appears that this model does slightly better than our earlier model, but still fails are adequately representing our data. We are failing to reproduce the multi-modality of the data. What could we do to improve our model?



### Aside: Centering

The intercept of a linear regression model is normally interpreted as the average value of $y$ when all predictors equal 0. A consequence of this definition means that the value of the intercept is frequently uninterpretable without also studying the regression coefficients. This is also the reason that we commonly need very weak priors for intercepts.

A trick for improving our interpretation of the intercept $\alpha$ is **centering** our (continuous) predictors. Centering is the procedure of subtracting the mean of a variable from each value. Namely:
```{r center}

pantheria <- 
  pantheria %>%
  mutate(mass_log_center = mass_log - mean(mass_log, na.rm = TRUE))

```

$\alpha$ is still the expected value of the outcome variable when the predictor is equal to zero. But now the mean value of the predictor is also zero. So the intercept all means: the expected value of the outcome, when the predictor is at its average value. This makes interpreting the intercept a lot easier.

To illustrate this, let's refit the model with the newly centered data.

```{r fit_center, cache = TRUE, message = FALSE, results = 'hide'}

m_3 <- brm(data = pantheria,
           family = gaussian(),
           formula = bf(density_log ~ 1 + mass_log_center),
           prior = c(prior(normal(0, 10), class = Intercept),
                     prior(normal(0, 10), class = b),
                     prior(cauchy(0, 5), class = sigma)),
           iter = 2000,
           warmup = 1000,
           chains = 4,
           cores = 4,
           refresh = 0)

```

```{r center_print}

print(m_3)

```

Can you explain why centering changes the value of $\alpha$ but not $\beta$? 

Centering will not change our models posterior predictive performance, but really improves the interpretability of our model parameters. Centering can also be beneficial for estimating parameter values by decreasing posterior correlation amoung the parameters.

I recommend always centering your (continuous) predictors.







# More than one predictor


$$
\begin{align}
y_{i} &\sim \text{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \beta_{1} m_{i} + \beta_{2} o_{i} + \beta_{3} n_{i} \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Normal}(0, 5) \\
\sigma &\sim \text{Cauchy}^{+}(0, 5) \\
\end{align}
$$


```{r fit_multi, cache = TRUE, message = FALSE, results = 'hide'}

m_4 <- brm(data = pantheria,
           family = gaussian(),
           formula = bf(density_log ~ 1 + mass_log_center + activity_cycle),
           prior = c(prior(normal(0, 10), class = Intercept),
                     prior(normal(0, 10), class = b),
                     prior(cauchy(0, 5), class = sigma)),
           iter = 2000,
           warmup = 1000,
           chains = 4,
           cores = 4,
           refresh = 0)

```



## Scaling


```{r scale}

pantheria <- 
  pantheria %>%
  drop_na(mass_log) %>%
  mutate(mass_log_scale = scale(mass_log, center = TRUE, scale = TRUE))

```


```{r fit_multiscale, cache = TRUE, message = FALSE, results = 'hide'}

m_4 <- brm(data = pantheria,
           family = gaussian(),
           formula = bf(density_log ~ 1 + mass_log_scale + activity_cycle),
           prior = c(prior(normal(0, 10), class = Intercept),
                     prior(normal(0, 10), class = b),
                     prior(cauchy(0, 5), class = sigma)),
           iter = 2000,
           warmup = 1000,
           chains = 4,
           cores = 4,
           refresh = 0)

```

